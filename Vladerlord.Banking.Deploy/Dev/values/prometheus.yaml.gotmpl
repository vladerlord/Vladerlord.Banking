alertmanager:
  podAnnotations:
    linkerd.io/inject: enabled
pushgateway:
  podAnnotations:
    linkerd.io/inject: enabled
nodeExporter:
  hostRootfs: false
server:
  podAnnotations:
    linkerd.io/inject: enabled
  extraFlags:
    - web.enable-lifecycle
    - web.route-prefix=/
    - web.external-url=http://{{ .Values.banking.devDomain }}/prometheus
    - storage.tsdb.no-lockfile
  ingress:
    enabled: true
    annotations:
      kubernetes.io/ingress.class: nginx
      nginx.ingress.kubernetes.io/rewrite-target: /$2
      nginx.ingress.kubernetes.io/ssl-redirect: "false"
      {{ if .Values.dev.enableOauth }}
      nginx.ingress.kubernetes.io/auth-url: https://{{ .Values.banking.devAuthDomain }}/oauth2/auth
      nginx.ingress.kubernetes.io/auth-signin: https://{{ .Values.banking.devAuthDomain }}/oauth2/start
      {{ end }}
    hosts:
      - {{ .Values.banking.devDomain }}
    path: /prometheus(/|$)(.*)

serverFiles:
  prometheus.yml:
    rule_files:
      - /etc/config/recording_rules.yml
      - /etc/config/alerting_rules.yml
      ## Below two files are DEPRECATED will be removed from this default values file
      - /etc/config/rules
      - /etc/config/alerts

    scrape_configs:
      - job_name: linkerd-jaeger-otel-collector
        scrape_interval: 10s
        static_configs:
          - targets: ['collector.linkerd-jaeger.svc.cluster.local:8889']
          - targets: ['collector.linkerd-jaeger.svc.cluster.local:8888']

      - job_name: prometheus
        static_configs:
          - targets:
              - localhost:9090

      - job_name: 'kubernetes-apiservers'
        kubernetes_sd_configs:
          - role: endpoints

        # Default to scraping over https. If required, just disable this or change to
        # `http`.
        scheme: https

        # This TLS & bearer token file config is used to connect to the actual scrape
        # endpoints for cluster components. This is separate to discovery auth
        # configuration because discovery & scraping are two separate concerns in
        # Prometheus. The discovery auth config is automatic if Prometheus runs inside
        # the cluster. Otherwise, more config options have to be provided within the
        # <kubernetes_sd_config>.
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          # If your node certificates are self-signed or use a different CA to the
          # master CA, then disable certificate verification below. Note that
          # certificate verification is an integral part of a secure infrastructure
          # so this should only be disabled in a controlled environment. You can
          # disable certificate verification by uncommenting the line below.
          #
          insecure_skip_verify: true
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

        # Keep only the default/kubernetes service endpoints for the https port. This
        # will add targets for each API server which Kubernetes adds an endpoint to
        # the default/kubernetes service.
        relabel_configs:
          - source_labels: [ __meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name ]
            action: keep
            regex: default;kubernetes;https

      - job_name: 'kubernetes-nodes'

        # Default to scraping over https. If required, just disable this or change to
        # `http`.
        scheme: https

        # This TLS & bearer token file config is used to connect to the actual scrape
        # endpoints for cluster components. This is separate to discovery auth
        # configuration because discovery & scraping are two separate concerns in
        # Prometheus. The discovery auth config is automatic if Prometheus runs inside
        # the cluster. Otherwise, more config options have to be provided within the
        # <kubernetes_sd_config>.
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          # If your node certificates are self-signed or use a different CA to the
          # master CA, then disable certificate verification below. Note that
          # certificate verification is an integral part of a secure infrastructure
          # so this should only be disabled in a controlled environment. You can
          # disable certificate verification by uncommenting the line below.
          #
          insecure_skip_verify: true
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

        kubernetes_sd_configs:
          - role: node

        relabel_configs:
          - action: labelmap
            regex: __meta_kubernetes_node_label_(.+)
          - target_label: __address__
            replacement: kubernetes.default.svc:443
          - source_labels: [ __meta_kubernetes_node_name ]
            regex: (.+)
            target_label: __metrics_path__
            replacement: /api/v1/nodes/$1/proxy/metrics

      #  Required for: https://grafana.com/grafana/dashboards/315
      - job_name: 'kubernetes-nodes-cadvisor'
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          insecure_skip_verify: true
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        kubernetes_sd_configs:
          - role: node
        relabel_configs:
          - action: labelmap
            regex: __meta_kubernetes_node_label_(.+)
          - target_label: __address__
            replacement: kubernetes.default.svc:443
          - source_labels: [ __meta_kubernetes_node_name ]
            regex: (.+)
            target_label: __metrics_path__
            replacement: /api/v1/nodes/$1/proxy/metrics/cadvisor
        metric_relabel_configs:
          - source_labels: [ __name__ ]
            regex: '(container|machine)_(cpu|memory|network|fs)_(.+)'
            action: keep
          - source_labels: [ __name__ ]
            regex: 'container_memory_failures_total' # unneeded large metric
            action: drop

      # Scrape config for service endpoints.
      #
      # The relabeling allows the actual service scrape endpoint to be configured
      # via the following annotations:
      #
      # * `prometheus.io/scrape`: Only scrape services that have a value of
      # `true`, except if `prometheus.io/scrape-slow` is set to `true` as well.
      # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need
      # to set this to `https` & most likely set the `tls_config` of the scrape config.
      # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
      # * `prometheus.io/port`: If the metrics are exposed on a different port to the
      # service then set this appropriately.
      # * `prometheus.io/param_<parameter>`: If the metrics endpoint uses parameters
      # then you can set any parameter
      - job_name: 'kubernetes-service-endpoints'
        honor_labels: true

        kubernetes_sd_configs:
          - role: endpoints

        relabel_configs:
          - source_labels: [ __meta_kubernetes_service_annotation_prometheus_io_scrape ]
            action: keep
            regex: true
          - source_labels: [ __meta_kubernetes_service_annotation_prometheus_io_scrape_slow ]
            action: drop
            regex: true
          - source_labels: [ __meta_kubernetes_service_annotation_prometheus_io_scheme ]
            action: replace
            target_label: __scheme__
            regex: (https?)
          - source_labels: [ __meta_kubernetes_service_annotation_prometheus_io_path ]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels: [ __address__, __meta_kubernetes_service_annotation_prometheus_io_port ]
            action: replace
            target_label: __address__
            regex: (.+?)(?::\d+)?;(\d+)
            replacement: $1:$2
          - action: labelmap
            regex: __meta_kubernetes_service_annotation_prometheus_io_param_(.+)
            replacement: __param_$1
          - action: labelmap
            regex: __meta_kubernetes_service_label_(.+)
          - source_labels: [ __meta_kubernetes_namespace ]
            action: replace
            target_label: namespace
          - source_labels: [ __meta_kubernetes_service_name ]
            action: replace
            target_label: service
          - source_labels: [ __meta_kubernetes_pod_node_name ]
            action: replace
            target_label: node

      # Scrape config for slow service endpoints; same as above, but with a larger
      # timeout and a larger interval
      #
      # The relabeling allows the actual service scrape endpoint to be configured
      # via the following annotations:
      #
      # * `prometheus.io/scrape-slow`: Only scrape services that have a value of `true`
      # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need
      # to set this to `https` & most likely set the `tls_config` of the scrape config.
      # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
      # * `prometheus.io/port`: If the metrics are exposed on a different port to the
      # service then set this appropriately.
      # * `prometheus.io/param_<parameter>`: If the metrics endpoint uses parameters
      # then you can set any parameter
      - job_name: 'kubernetes-service-endpoints-slow'
        honor_labels: true

        scrape_interval: 5m
        scrape_timeout: 30s

        kubernetes_sd_configs:
          - role: endpoints

        relabel_configs:
          - source_labels: [ __meta_kubernetes_service_annotation_prometheus_io_scrape_slow ]
            action: keep
            regex: true
          - source_labels: [ __meta_kubernetes_service_annotation_prometheus_io_scheme ]
            action: replace
            target_label: __scheme__
            regex: (https?)
          - source_labels: [ __meta_kubernetes_service_annotation_prometheus_io_path ]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels: [ __address__, __meta_kubernetes_service_annotation_prometheus_io_port ]
            action: replace
            target_label: __address__
            regex: (.+?)(?::\d+)?;(\d+)
            replacement: $1:$2
          - action: labelmap
            regex: __meta_kubernetes_service_annotation_prometheus_io_param_(.+)
            replacement: __param_$1
          - action: labelmap
            regex: __meta_kubernetes_service_label_(.+)
          - source_labels: [ __meta_kubernetes_namespace ]
            action: replace
            target_label: namespace
          - source_labels: [ __meta_kubernetes_service_name ]
            action: replace
            target_label: service
          - source_labels: [ __meta_kubernetes_pod_node_name ]
            action: replace
            target_label: node

      - job_name: 'prometheus-pushgateway'
        honor_labels: true

        kubernetes_sd_configs:
          - role: service

        relabel_configs:
          - source_labels: [ __meta_kubernetes_service_annotation_prometheus_io_probe ]
            action: keep
            regex: pushgateway

      # Example scrape config for pods
      #
      # The relabeling allows the actual pod scrape endpoint to be configured via the
      # following annotations:
      #
      # * `prometheus.io/scrape`: Only scrape pods that have a value of `true`,
      # except if `prometheus.io/scrape-slow` is set to `true` as well.
      # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need
      # to set this to `https` & most likely set the `tls_config` of the scrape config.
      # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
      # * `prometheus.io/port`: Scrape the pod on the indicated port instead of the default of `9102`.
      - job_name: 'kubernetes-pods'
        honor_labels: true

        kubernetes_sd_configs:
          - role: pod
            namespaces:
              names: [ 'default' ]

        relabel_configs:
          - source_labels: [ __meta_kubernetes_pod_annotation_prometheus_io_scrape ]
            action: keep
            regex: true
          - source_labels: [ __meta_kubernetes_pod_annotation_prometheus_io_scrape_slow ]
            action: drop
            regex: true
          - source_labels: [ __meta_kubernetes_pod_annotation_prometheus_io_scheme ]
            action: replace
            regex: (https?)
            target_label: __scheme__
          - source_labels: [ __meta_kubernetes_pod_annotation_prometheus_io_path ]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels: [ __address__, __meta_kubernetes_pod_annotation_prometheus_io_port ]
            action: replace
            regex: (.+?)(?::\d+)?;(\d+)
            replacement: $1:$2
            target_label: __address__
          - action: labelmap
            regex: __meta_kubernetes_pod_annotation_prometheus_io_param_(.+)
            replacement: __param_$1
          - action: labelmap
            regex: __meta_kubernetes_pod_label_(.+)
          - source_labels: [ __meta_kubernetes_namespace ]
            action: replace
            target_label: namespace
          - source_labels: [ __meta_kubernetes_pod_name ]
            action: replace
            target_label: pod
          - source_labels: [ __meta_kubernetes_pod_phase ]
            regex: Pending|Succeeded|Failed|Completed
            action: drop

      # > Linkerd section
      - job_name: 'grafana'
        scrape_interval: 10s
        scrape_timeout: 10s
        kubernetes_sd_configs:
          - role: pod
            namespaces:
              names: [ 'linkerd-viz' ]
        relabel_configs:
          - source_labels:
              - __meta_kubernetes_pod_container_name
            action: keep
            regex: ^grafana$

      - job_name: 'linkerd-controller'
        scrape_interval: 10s
        scrape_timeout: 10s
        kubernetes_sd_configs:
          - role: pod
            namespaces:
              names:
                - 'linkerd'
                - 'linkerd-viz'
        relabel_configs:
          - source_labels:
              - __meta_kubernetes_pod_container_port_name
            action: keep
            regex: admin-http
          - source_labels: [ __meta_kubernetes_pod_container_name ]
            action: replace
            target_label: component

      - job_name: 'linkerd-service-mirror'
        scrape_interval: 10s
        scrape_timeout: 10s
        kubernetes_sd_configs:
          - role: pod
        relabel_configs:
          - source_labels:
              - __meta_kubernetes_pod_label_linkerd_io_control_plane_component
              - __meta_kubernetes_pod_container_port_name
            action: keep
            regex: linkerd-service-mirror;admin-http$
          - source_labels: [ __meta_kubernetes_pod_container_name ]
            action: replace
            target_label: component

      - job_name: 'linkerd-proxy'
        scrape_interval: 10s
        scrape_timeout: 10s
        kubernetes_sd_configs:
          - role: pod
        relabel_configs:
          - source_labels:
              - __meta_kubernetes_pod_container_name
              - __meta_kubernetes_pod_container_port_name
              - __meta_kubernetes_pod_label_linkerd_io_control_plane_ns
            action: keep
            regex: ^linkerd-proxy;linkerd-admin;linkerd$
          - source_labels: [ __meta_kubernetes_namespace ]
            action: replace
            target_label: namespace
          - source_labels: [ __meta_kubernetes_pod_name ]
            action: replace
            target_label: pod
          # special case k8s' "job" label, to not interfere with prometheus' "job"
          # label
          # __meta_kubernetes_pod_label_linkerd_io_proxy_job=foo =>
          # k8s_job=foo
          - source_labels: [ __meta_kubernetes_pod_label_linkerd_io_proxy_job ]
            action: replace
            target_label: k8s_job
          # drop __meta_kubernetes_pod_label_linkerd_io_proxy_job
          - action: labeldrop
            regex: __meta_kubernetes_pod_label_linkerd_io_proxy_job
          # __meta_kubernetes_pod_label_linkerd_io_proxy_deployment=foo =>
          # deployment=foo
          - action: labelmap
            regex: __meta_kubernetes_pod_label_linkerd_io_proxy_(.+)
          # drop all labels that we just made copies of in the previous labelmap
          - action: labeldrop
            regex: __meta_kubernetes_pod_label_linkerd_io_proxy_(.+)
          # __meta_kubernetes_pod_label_linkerd_io_foo=bar =>
          # foo=bar
          - action: labelmap
            regex: __meta_kubernetes_pod_label_linkerd_io_(.+)
          # Copy all pod labels to tmp labels
          - action: labelmap
            regex: __meta_kubernetes_pod_label_(.+)
            replacement: __tmp_pod_label_$1
          # Take `linkerd_io_` prefixed labels and copy them without the prefix
          - action: labelmap
            regex: __tmp_pod_label_linkerd_io_(.+)
            replacement: __tmp_pod_label_$1
          # Drop the `linkerd_io_` originals
          - action: labeldrop
            regex: __tmp_pod_label_linkerd_io_(.+)
          # Copy tmp labels into real labels
          - action: labelmap
            regex: __tmp_pod_label_(.+)
      # < Linkerd section
